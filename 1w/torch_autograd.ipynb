{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34745e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\joohy/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# resent18 모델 불러오기\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# data = 3채널, 높이 64, 넓이 64\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000) # 라벨 데이터는 (1,1000) 모양\n",
    "\n",
    "# 순전파 단계(forward)\n",
    "prediction = model(data) # prediction\n",
    "\n",
    "loss = (prediction-labels).sum()  # loss 계산\n",
    "loss.backward() # 역전파 단계(backward)\n",
    "\n",
    "# 옵티마이저 불러오기\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9) # 0.1 학습률, 0.9 모멘텀\n",
    "# 모멘텀은 현재 지점의 경사도 + 이전까지의 진행 방향(관성)을 함께 고려함/\n",
    "# 0.9는 이전 업데이트 방향을 90% 반영하겠다. 학습 속도 향상 + 작은 언덕에서 빠져나올 확률이 큼 + 안정적인 학습\n",
    "optim.step() # 경사하강법(gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ef9ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([False, False])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2.,3.], requires_grad=True) # requires_grad 모든 연산 추적 여부\n",
    "b = torch.tensor([6.,4.], requires_grad=True)\n",
    "\n",
    "# 새로운 텐서 만들기\n",
    "Q = 3*a**3-b**2\n",
    "\n",
    "# Q가 오차(error)라고 가정, Q에 대해서 .backward()를 호출하면, autograd는 변화도를 계산하고 .grad 속성(atiiribute)에 저장한다.\n",
    "external_grad = torch.tensor([1.,1.])\n",
    "Q.backward(gradient=external_grad) # loss가 스칼라 값이 아니면 gradient 명시해야함\n",
    "\n",
    "# 변화도는 a.grad 와 b.grad에 저장됨\n",
    "print(9*a**2 == a.grad)\n",
    "print(12*b == b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bc86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients?: False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "# autograd는 연산들의 기록을 방향성 비순환 그래프(DAG에 저장)\n",
    "# DAG의 leave는 입력 텐서, root는 결과 텐서, 뿌리에서부터 잎까지 추적하면 연쇄 법칙(chain rule)에 따라 변화도를 자동으로 계산\n",
    "\n",
    "# 순전파 단계에서 autograd는 결과 텐서 계산, DAG에 변화도 기능(gradient function)을 유지한다.\n",
    "# 역전파 단계에서 DAG 뿌리에서 .backward()가 호출될 때 시작됨, autograd는 각 .grad_fn으로 부터 변화도를 계산하고, 각 텐서의 .grad 속성에 계산 결과를 쌓고(accumulate), 연쇄 법칙을 사용하여, 모든 leaf 텐서들까지 전파한다.\n",
    "\n",
    "# requires_grad가 True면 모든 텐서에 대한 연산들을 추적한다. 입력 텐서 중 하나라도 True면 연산 결과 텐서도 변화도를 갖음.\n",
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients?: {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")\n",
    "\n",
    "\n",
    "# 변화도를 계산하지 않는 매개변수를 일반적으로 고정된 매개변수(frozen parameter)라고 함.\n",
    "# 변화도가 필요없으면 고정하는 것이 가능. fine-tuning에서 새로운 정답을 예측할 수 있도록 모델의 대부분을 고정한 뒤 일반적으로 분류 계층(classifier layer)만 변경함 (여러 방법 중 하나)\n",
    "from torch import nn,optim\n",
    "\n",
    "# 모델 불러오기\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# 신경망의 모든 매개변수를 고정함.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 10개의 정답을 갖는 새로운 데이터셋으로 모델을 미세조정\n",
    "# resnet에서 classifier은 마지막 선형 계층(linear layer)인 model.fc으로 초기화\n",
    "model.fc = nn.Linear(512, 10)\n",
    "\n",
    "# 마지막 분류기만 최적화 대상으로 삼음\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
